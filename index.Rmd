---
title: "O uso do IRaMuTeQ para análise de decisões judiciais"
subtitle: "Exemplo de aplicação nas Ações de Investigação Judicial Eleitoral do Tribunal Superior Eleitoral"
description: |
  Esse site é destinado a demonstrar o procedimento utilizado na pesquisa a ser apresentada no V Congresso Internacional de Direito Constitucional e Filosofia Politica.
author:
  - name: Jonathan Morais Barcellos Ferreira
    affiliation: Universidade Federal do Rio Grande
  - name: Gabriel Delias de Sousa Simões
    affiliation: Universidade Federal do Rio Grande
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

O código utilizado pode ser encontrado [aqui](SCRIPT/), e os dados [aqui](DATA/).

# Coleta das decisões judiciais

O primeiro passo é coletar os dados de interesse: as decisões do Tribunal Superior Eleitoral (TSE) as Ações de Investigação Judicial Eleitoral (AIJE). O portal de jurisprudência do TSE possui reCaptcha o que nos impediu de realizar a raspagem direta no site. Por isso, criamos um arquivo `json` com os dados extraído do portal.

A busca no [portal de jurisprudência do TSE](https://jurisprudencia.tse.jus.br/#/jurisprudencia/pesquisa) foi feita com os seguintes parâmetros: Classes: AIJE; Acórdão: verdadeiro; e Resolução: verdadeiro.

Após criar o arquivo [`aije_tse.json`](DATA/aije_tse.json) fizemos a leitura e a criação de uma tabela com os dados que nos interessava.

```{r}
dados <- jsonlite::read_json("DATA/aije_tse.json")

tabela_dados <- purrr::map_dfr(seq_along(dados$content), ~ {
  tabela_dados <- data.frame(
    cod_dec = dados$content[[.x]]$codigoDecisao,
    data_dec = dados$content[[.x]]$dataDecisao,
    sig_c = dados$content[[.x]]$siglaClasse,
    desc_c = dados$content[[.x]]$descricaoClasse,
    tipo_proc = dados$content[[.x]]$nomeTipoProcesso,
    relator = dados$content[[.x]]$relator[[1]]$nome
  )
}, .progress = TRUE)
```

Como nos interessava apenas as decisões tomadas a partir de 2018, filtramos os dados, criamos uma variável apenas com o ano e deletamos o sobrenome dos Ministros:

```{r}
tabela_dados <-
  tabela_dados |> dplyr::filter(
    lubridate::year(as.Date(data_dec, "%d/%m/%Y")) >= 2018
  )

tabela_dados$data_dec <- lubridate::year(lubridate::dmy(tabela_dados$data_dec))

tabela_dados$relator <- stringr::str_extract(tabela_dados$relator, "\\b\\w+")
```

Com isso obtivemos a seguinte tabela:

```{r, eval=TRUE, echo=FALSE}
readRDS("DATA/backup_dados.rds") |>
  DT::datatable()
```

A tabela contém o código da decisão que foi utilizado para baixar os arquivos

```{r}
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.51" # nolint: line_length_linter.

purrr::walk(seq(1, nrow(tabela_dados)), purrr::possibly(~ {
  response <- httr::GET(
    paste0(
      "https://sjur-servicos.tse.jus.br/sjur-servicos/rest/download/pdf/",
      tabela_dados$cod_dec[[.x]]
    ),
    httr::add_headers("User-Agent" = ua),
    httr::write_disk(
      paste0("DATA/DECISOES/", tabela_dados$cod_dec[[.x]], ".pdf"),
      TRUE
    )
  )}))
```

Os arquivos foram salvos na pasta [`DATA/DECISOES`](DATA/DECISOES/).

# Tratamento das decisões judiciais

Após coletadas as decisões, tratamos os dados para melhor aproveitamento do texto pelo software IRaMuTeQ. Como medida  de recorte do texto para análise do software, removemos os cabeçalhos e rodapés. O primeiro passo para tal é listar os arquivos PDF presentes na pasta "DATA/DECISOES/".

```{r}
pdf <- list.files("DATA/DECISOES/", pattern = "*.pdf", full.names = TRUE)
```

Os arquivos foram previamente classificados em 2 tipos de configurações rodapé: A - com 2 linhas de rodapé - e D - com 1 linha de rodapé - além de 2 tipos de configurações de cabeçalho: B - 4 linhas de cabeçalho, C - 1 linha de cabeçalho. Após listados, os arquivos foram submetidos ao recorte de seus cabeçalhos e rodapés de acordo com sua classificação previamente estabelescida.

Para isso, utilizou-se da função "purrr::map_df" que aplica a todos elementos da lista de decisões em PDF a função "tabulizer::extract_text", que extrai o texto de um arquivo PDF, e os une em um único dataframe. A função "lapply" aplica uma função anômima, defenida posteriormente, em cada página do acórdão. A  função anônima function(pagina) é definida para processar cada página do acórdão em PDF. Essa função é composta por diversas etapas:

Extração do texto da página: A função "tabulizer::extract_text(.x, 1:pdftools::pdf_length((.x)) )" é utilizada para extrair o texto da página atual (pagina). Essa função retorna um vetor de strings, onde cada elemento representa uma linha do texto.

Identificação do tipo de configuração: A função "stringr::str_detect(.x, "_B") == TRUE" verifica se o nome do arquivo PDF contém a string "_B". Se verdadeiro, significa que o arquivo possui 4 linhas de cabeçalho e 2 linhas de rodapé (configuração tipo B).

Remoção do cabeçalho e rodapé: Para o  tipo B, o código define "rodape" como 2 e remove as últimas 2 linhas do vetor de strings (linhas). Isso resulta em um novo vetor (linhas_tratadas) que contém apenas o texto principal da página.

O código utiliza lógica similar para identificar as configurações tipo A, C e D e remover as linhas de cabeçalho e rodapé de acordo com cada tipo.

A função paste(linhas_tratadas, collapse = "\n") junta todas as linhas do vetor linhas_tratadas em um único texto, utilizando um caractere de nova linha (\n) como separador.

```{r}
tabela <- purrr::map_df(pdf, ~ {
  texto <- tabulizer::extract_text(
    .x,
    1:pdftools::pdf_length((.x))
  )
  texto_tratado <- lapply(texto, function(pagina) {
    linhas <- strsplit(pagina, "\n")[[1]]

    if (stringr::str_detect(.x, "_B") == TRUE) {
      # arquivos tipo B tem apenas 4 linhas de cabeçalho
      rodape <- 2
      linhas_tratadas <-
        linhas[-seq(
          length(linhas),
          length(linhas) - min(rodape, length(linhas))
        )]
    } else if (stringr::str_detect(.x, "_A") == TRUE) {
      # arquivos tipo A tem 2 linhas de rodapé
      rodape <- 1
      linhas_tratadas <-
        linhas[-seq(
          length(linhas),
          length(linhas) - min(rodape, length(linhas))
        )]
    } else if (stringr::str_detect(.x, "_E") == TRUE) {
      # arquivos tipo A tem 2 linhas de rodapé
      cabecalho <- 4
      linhas_tratadas <-
        linhas[-c(1:cabecalho)]
    } else if (stringr::str_detect(.x, "_C") == TRUE) {
      # arquivos tipo C tem 1 linha de cabeçalho
      linhas_tratadas <-
        linhas[-1]
    } else if (stringr::str_detect(.x, "_D") == TRUE) {
      # arquivos tipo D tem 1 linha de rodapé
      linhas_tratadas <-
        linhas[-length(linhas)]
    }

    texto_novo <- paste(linhas_tratadas, collapse = "\n")
    return(texto_novo)
  })
  data <- data.frame(
    doc = stringr::str_extract(.x, "\\d+"),
    text = paste0(texto_tratado, collapse = "\n")
  )
}, .progress = TRUE)

```

A variável "tabela" é um dateframe com os resultados da função "purrr::map_df", que reune os condicionais aplicados a todas as páginas de todos os PDFs.

Para melhor eficiência durante o processo de desenvolvimento do código e execução do script, optamos por criar o arquivo "tabela.rds", salvo no diretório "DATA" para evitar a execução redundante do código de remoção de cabeçalhos e rodapés. Adicionamos a função "readRDS" para ler o arquivo de backup criado.

```{r}
saveRDS(tabela, "DATA/tabela.rds")
tabela <- readRDS("DATA/tabela.rds")
```

Posteriormente, removemos o relatório da ação, presente na coluna "text" do dataframa "tabela". O relatório precede a palavra "VOTO", que dá início ao voto do Ministro relator e removido utilizando manipulação de strings.

A função stringr::str_locate() identifica a posição da primeira ocorrência da palavra-chave "VOTO" no texto. O argumento "\\r" indica que a pesquisa considera quebras de linha.

Se a palavra-chave "VOTO" for encontrada, o código remove o texto anterior à sua posição, incluindo 4 caracteres adicionais para garantir a remoção completa da palavra-chave. A função "substr()" é utilizada para extrair uma sub-string do texto original.

Se a palavra-chave "VOTO" for encontrada sem quebra de linha, o código utiliza uma regex alternativa que considera espaços em branco "\\s". O texto anterior à palavra-chave é removido da mesma forma que no caso anterior.

```{r}
tabela$text <- purrr::map(seq_along(tabela$text), ~{
  posicao <- stringr::str_locate(
    tabela$text[[.x]],
    "VOTO\\r"
  )[1]
  if (!is.na(posicao)) {
    tabela$text[[.x]] <- substr(
      tabela$text[[.x]],
      posicao + 4,
      nchar(tabela$text[[.x]])
  )} else {
    posicao <- stringr::str_locate(
      tabela$text[[.x]],
      "VOTO\\s"
    )[1]
    tabela$text[[.x]] <- tabela$text[[.x]] |>
    substr(posicao + 4, nchar(tabela$text[[.x]]))
  }
})

tabela$text <- as.character(tabela$text)
```

Por fim, o código converte a coluna text para o tipo de dado character.

Em seguida, utilizamos da função "quanteda::corpus' para criar o corpus "tse_corpus", para a análise específica de texto a partir do dataframe "tabela" criado anteriormente e definido pela função "purrr:map_df" e modificado pela remoção do relatório.

```{r}
tse_corpus <- quanteda::corpus(
  tabela,
  docid_field = "doc",
  text_field = "text"
)
```

A partir do corpus criado previamente, utilizamso da função "quanteda::tokens" para processar o corpus "tse_corpus", gerando tokens, unidades básicas do texto, que facilitam a análise do texto pelo software.

```{r}
tse_tokens <-
  quanteda::tokens(
    tse_corpus,
    remove_punct = T,
    remove_symbols = T,
    remove_url = T,
    remove_separators = T,
    remove_numbers = T,
    split_hyphens = T
  ) |>
  quanteda::tokens_tolower()
```

Além de gerar os tokens, a função remove pontuações, símbolos, como porcentagem, cifrão, entre outros, endereços de internet, separadores, como quebras de página, números e separa palavras hifenizadas em tokens seoarados. Por fim, descapitaliza todas as palavras, colocando-as em sua forma minúscula.

Em seguida, buscamos aplicar dicionários para unifomrizar a grafia de certos termos e unificar certos tokens, como nomes próprios. Para tal, utiliza-se a função "read.delim" para ler o dicionário em formato de arquivo de texto (txt).

```{r}
dicionario <- read.delim("DATA/DIC/dicionario.txt", header = FALSE)
tse_tokens <-
  quanteda::tokens_compound(
    tse_tokens,
    pattern = quanteda::phrase(dicionario$V1)
  )
tse_tokens |>
  decJ::dicionario.Criar(
    n = 5,
    "DATA/DIC/"
  )
```

Logo depois, utiliza-se a função "quanteda::tokens_compound" para combinar tolens que formam expressões regulares compostas. O argumento "pattern" especifica a expressão regular que define as expressões compostas. Neste caso, a coluna V1 é utilizada como referência de padrões para expressões comuns.

A função "decJ::dicionario.Criar" é utilizada para criar dicionários de contexto a partir dos tokens. O argumento "n" especifica o tamanho do contexto (palavras antes e depois da palavra central). O argumento "DATA/DIC/" espeifica o diretório da pasta raiz do projeto onde os dicionários serão salvos.

```{r}
tabela_dados <- readRDS("DATA/backup_dados.rds")
```

Para obter as variáveis para classificação dentro do arquivo a ser analisado pelo software, se lê o arquivo "backup_dados.rds", salvo ainda na fase de coleta.

Para finalizar o tratamento do texto, utiliza-se a função "sapply()" para aplicação da função "paste" a cada token do corpus, concatenando-os em uma única string. O resultado é convertido em um dataframa com uma coluna para o código do documento e outra para o texto tratado.

```{r}
iramuteq <- sapply(tse_tokens, paste, collapse = " ") |> as.data.frame() |> tibble::rownames_to_column()

names(iramuteq) <- c('cod_dec', 'tratado')
```

Na penúltima etapa, adicionamos uma colun a tabela com os metadados da tabela sobre os documentos, incluindo o código da decisão, relator e ano. Por fim, reordena as colunas da tabela para que os metadados sejam os primeiros.

```{r}
iramuteq <-
  iramuteq |>
  dplyr::mutate(
    codigo = paste0(
      "\n****",
      " *dec_",
      cod_dec,
      " *rel_",
      tabela_dados$relator,
      " *ano_",
      tabela_dados$data_dec
    )
  ) |>
  dplyr::relocate(codigo, tratado)
```

Ao fim, a função "readr::write_delim()" salva o dataframe no formato de texto delimitado (txt), com o nome "iramuteq_2.txt", utilizando o caractere de nova linha "\n" como separador. As colunas não são salvas e as aspas simples não são utilizadas para evitar erros de formatação no Iramuteq.

# IRaMuTeQ

## Inserção do txt, configurações iniciais de análise, Reinert - CHD, AFC. 

Após finalizada a etapa do tratamento de texto, para a realização das análises do corpus tratato pelo software, adicionamos a 